{"nbformat":4,"nbformat_minor":5,"metadata":{"accelerator":"GPU","colab":{"name":"object_detection_frcnn_mscoco_inference.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"cells":[{"cell_type":"code","metadata":{"id":"-TF52KB8b4oX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647902724280,"user_tz":240,"elapsed":7383,"user":{"displayName":"S.M. Hasan Mansur","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17673291696270669151"}},"outputId":"8a49bcdd-f427-416c-e1e3-5a00d73bc2f5"},"source":["# install pytorch \n","!pip install --pre torch torchvision torchaudio -f https://download.pytorch.org/whl/nightly/cu102/torch_nightly.html -U"],"id":"-TF52KB8b4oX","execution_count":109,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in links: https://download.pytorch.org/whl/nightly/cu102/torch_nightly.html\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.12.0.dev20220321+cu102)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.13.0.dev20220321+cu102)\n","Requirement already satisfied: torchaudio in /usr/local/lib/python3.7/dist-packages (0.12.0.dev20220321+cu102)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.10.0.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.21.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision) (2.23.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2.10)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ox6ecnFOZXVf","executionInfo":{"status":"ok","timestamp":1647902734308,"user_tz":240,"elapsed":826,"user":{"displayName":"S.M. Hasan Mansur","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17673291696270669151"}},"outputId":"e0d4240f-9a36-43d8-918d-2871b5a5e992"},"source":["# test torch version and CUDA device\n","import torch\n","print(torch.__version__)\n","a = torch.Tensor([1]).cuda()\n","print(a)"],"id":"Ox6ecnFOZXVf","execution_count":110,"outputs":[{"output_type":"stream","name":"stdout","text":["1.12.0.dev20220321+cu102\n","tensor([1.], device='cuda:0')\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jqi92Iipg4Oi","executionInfo":{"status":"ok","timestamp":1647902739321,"user_tz":240,"elapsed":2043,"user":{"displayName":"S.M. Hasan Mansur","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17673291696270669151"}},"outputId":"d90d1074-f3ee-4df2-dab7-8db1b9a260a5"},"source":["# mount google drive\n","from google.colab import drive\n","drive.mount(\"/content/gdrive\")"],"id":"Jqi92Iipg4Oi","execution_count":112,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}]},{"cell_type":"code","metadata":{"id":"7GxwWo5hhDNG","executionInfo":{"status":"ok","timestamp":1647902741269,"user_tz":240,"elapsed":309,"user":{"displayName":"S.M. Hasan Mansur","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17673291696270669151"}}},"source":["# change to project home directory\n","import os\n","os.chdir(\"/content/gdrive/My Drive/object_detection_frcnn_mscoco_boilerplate\")"],"id":"7GxwWo5hhDNG","execution_count":113,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"DNx5XDfcXGHe","executionInfo":{"status":"ok","timestamp":1647902743993,"user_tz":240,"elapsed":265,"user":{"displayName":"S.M. Hasan Mansur","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17673291696270669151"}},"outputId":"865aeac8-87bf-4941-b7a8-6bf847ae1cd4"},"source":["pwd"],"id":"DNx5XDfcXGHe","execution_count":114,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/gdrive/My Drive/object_detection_frcnn_mscoco_boilerplate'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":114}]},{"cell_type":"code","metadata":{"id":"7f44a319","executionInfo":{"status":"ok","timestamp":1647902745985,"user_tz":240,"elapsed":7,"user":{"displayName":"S.M. Hasan Mansur","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17673291696270669151"}}},"source":["import os\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torchvision\n","from torchvision import datasets, transforms, models\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor"],"id":"7f44a319","execution_count":115,"outputs":[]},{"cell_type":"code","metadata":{"id":"09e4e3e0","executionInfo":{"status":"ok","timestamp":1647902748663,"user_tz":240,"elapsed":8,"user":{"displayName":"S.M. Hasan Mansur","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17673291696270669151"}}},"source":["import utils\n"],"id":"09e4e3e0","execution_count":116,"outputs":[]},{"cell_type":"code","source":["import pprint\n","pp = pprint.PrettyPrinter()\n","import json"],"metadata":{"id":"8tlOF7KK9Cio","executionInfo":{"status":"ok","timestamp":1647902751324,"user_tz":240,"elapsed":2,"user":{"displayName":"S.M. Hasan Mansur","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17673291696270669151"}}},"id":"8tlOF7KK9Cio","execution_count":117,"outputs":[]},{"cell_type":"code","metadata":{"id":"118d0c34","executionInfo":{"status":"ok","timestamp":1647902757552,"user_tz":240,"elapsed":7,"user":{"displayName":"S.M. Hasan Mansur","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17673291696270669151"}}},"source":["# path to data and annotation\n","data_dir = 'inference_data'"],"id":"118d0c34","execution_count":121,"outputs":[]},{"cell_type":"code","metadata":{"id":"fe2bd16f","executionInfo":{"status":"ok","timestamp":1647902759424,"user_tz":240,"elapsed":6,"user":{"displayName":"S.M. Hasan Mansur","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17673291696270669151"}}},"source":["test_transforms = transforms.Compose([transforms.Resize(224),\n","                                      transforms.ToTensor(),\n","                                      #transforms.Normalize([0.485, 0.456, 0.406],\n","                                      #                     [0.229, 0.224, 0.225])\n","                                     ])\n","dataset_test = datasets.ImageFolder(data_dir, transform=test_transforms)"],"id":"fe2bd16f","execution_count":122,"outputs":[]},{"cell_type":"code","metadata":{"id":"1ab70728","executionInfo":{"status":"ok","timestamp":1647902765746,"user_tz":240,"elapsed":7,"user":{"displayName":"S.M. Hasan Mansur","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17673291696270669151"}}},"source":["data_loader_test = torch.utils.data.DataLoader(\n","    dataset_test, batch_size=1, shuffle=False, num_workers=1,\n","    collate_fn=utils.collate_fn)\n","\n"],"id":"1ab70728","execution_count":124,"outputs":[]},{"cell_type":"code","metadata":{"id":"21996757","executionInfo":{"status":"ok","timestamp":1647902768904,"user_tz":240,"elapsed":301,"user":{"displayName":"S.M. Hasan Mansur","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17673291696270669151"}}},"source":["# select device (whether GPU or CPU)\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"],"id":"21996757","execution_count":125,"outputs":[]},{"cell_type":"code","metadata":{"id":"f0e6ce42","executionInfo":{"status":"ok","timestamp":1647902774964,"user_tz":240,"elapsed":312,"user":{"displayName":"S.M. Hasan Mansur","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17673291696270669151"}}},"source":["num_classes = 6"],"id":"f0e6ce42","execution_count":128,"outputs":[]},{"cell_type":"code","source":["model = torchvision.models.detection.fasterrcnn_resnet50_fpn()\n","in_features = model.roi_heads.box_predictor.cls_score.in_features\n","model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n","model.load_state_dict(torch.load('chkpnt1.pth'))"],"metadata":{"id":"WvHFSK-V14_f","executionInfo":{"status":"ok","timestamp":1647902786330,"user_tz":240,"elapsed":314,"user":{"displayName":"S.M. Hasan Mansur","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17673291696270669151"}}},"id":"WvHFSK-V14_f","execution_count":134,"outputs":[]},{"cell_type":"code","source":["model.eval()"],"metadata":{"id":"KOZSKfMe2QAf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647902796867,"user_tz":240,"elapsed":3,"user":{"displayName":"S.M. Hasan Mansur","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17673291696270669151"}},"outputId":"5619a2a2-854d-403f-d2c0-572190409e82"},"id":"KOZSKfMe2QAf","execution_count":139,"outputs":[{"output_type":"execute_result","data":{"text/plain":["FasterRCNN(\n","  (transform): GeneralizedRCNNTransform(\n","      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n","  )\n","  (backbone): BackboneWithFPN(\n","    (body): IntermediateLayerGetter(\n","      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","      (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n","      (relu): ReLU(inplace=True)\n","      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","      (layer1): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(64, eps=1e-05)\n","          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(256, eps=1e-05)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (1): FrozenBatchNorm2d(256, eps=1e-05)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(64, eps=1e-05)\n","          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(256, eps=1e-05)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(64, eps=1e-05)\n","          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(256, eps=1e-05)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (layer2): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): FrozenBatchNorm2d(512, eps=1e-05)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (3): Bottleneck(\n","          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (layer3): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): FrozenBatchNorm2d(1024, eps=1e-05)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (3): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (4): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (5): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (layer4): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(512, eps=1e-05)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(512, eps=1e-05)\n","          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(2048, eps=1e-05)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): FrozenBatchNorm2d(2048, eps=1e-05)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(512, eps=1e-05)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(512, eps=1e-05)\n","          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(2048, eps=1e-05)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(512, eps=1e-05)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(512, eps=1e-05)\n","          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(2048, eps=1e-05)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","    )\n","    (fpn): FeaturePyramidNetwork(\n","      (inner_blocks): ModuleList(\n","        (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n","        (1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n","        (2): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n","        (3): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n","      )\n","      (layer_blocks): ModuleList(\n","        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      )\n","      (extra_blocks): LastLevelMaxPool()\n","    )\n","  )\n","  (rpn): RegionProposalNetwork(\n","    (anchor_generator): AnchorGenerator()\n","    (head): RPNHead(\n","      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n","      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n","    )\n","  )\n","  (roi_heads): RoIHeads(\n","    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n","    (box_head): TwoMLPHead(\n","      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n","      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n","    )\n","    (box_predictor): FastRCNNPredictor(\n","      (cls_score): Linear(in_features=1024, out_features=6, bias=True)\n","      (bbox_pred): Linear(in_features=1024, out_features=24, bias=True)\n","    )\n","  )\n",")"]},"metadata":{},"execution_count":139}]},{"cell_type":"code","source":["print(\"================= INFERENCE ===========================\")\n","with torch.no_grad():\n","    correct = 0\n","    total = 0\n","    inference_result = {}\n","    # for images, labels in data_loader_test:\n","    for i, (images, labels) in enumerate(data_loader_test, 0):\n","        print(\"================== batch ==================\")\n","        sample_fname, _ = data_loader_test.dataset.samples[i]\n","        filename = sample_fname.split(\"/\")[2]\n","        print(\"filename: \", filename)\n","        outputs = model(images)\n","        print(\"outputs\")\n","        pp.pprint(outputs)\n","\n","        # init boxes, labels, scores\n","        boxes = None\n","        labels = None\n","        scores = None\n","        \n","        # get boxes, labels, scores\n","        if len(outputs[0][\"boxes\"]) != 0:\n","          boxes = outputs[0][\"boxes\"][0].numpy().tolist()\n","        if len(outputs[0][\"labels\"]) != 0:\n","          labels = outputs[0][\"labels\"][0].numpy().tolist()\n","        if len(outputs[0][\"scores\"]) != 0:\n","          scores = outputs[0][\"scores\"][0].numpy().tolist()\n","        \n","        # populate JSON\n","        inference_result[\"boxes\"] = boxes\n","        inference_result[\"labels\"] = labels\n","        inference_result[\"scores\"] = scores\n","\n","        print(\"inference result\")\n","        pp.pprint(inference_result)\n","\n","        # write inference result\n","        inference_output_dir = \"./inference_output/\"\n","        inference_output_filename = inference_output_dir + filename.split(\".\")[0] + \".json\"\n","        with open(inference_output_filename, \"w\") as fp:\n","          json.dump(inference_result, fp)\n","        fp.close()\n","        # break"],"metadata":{"id":"xbDXfDvs5CeO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647912328081,"user_tz":240,"elapsed":107860,"user":{"displayName":"S.M. Hasan Mansur","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17673291696270669151"}},"outputId":"2dfae6e6-5195-438b-e2a4-d9057663cfab"},"id":"xbDXfDvs5CeO","execution_count":157,"outputs":[{"output_type":"stream","name":"stdout","text":["================= INFERENCE ===========================\n","================== batch ==================\n","filename:  communication_1--messenger-text-and-video-0-11_5c9f.jpg\n","outputs\n","[{'boxes': tensor([], size=(0, 4)),\n","  'labels': tensor([], dtype=torch.int64),\n","  'scores': tensor([])}]\n","inference result\n","{'boxes': None, 'labels': None, 'scores': None}\n","================== batch ==================\n","filename:  communication_1--messenger-text-and-video-1-25_9979.jpg\n","outputs\n","[{'boxes': tensor([[138.6384,  66.6878, 182.4153, 138.3146]]),\n","  'labels': tensor([3]),\n","  'scores': tensor([0.2108])}]\n","inference result\n","{'boxes': [138.6383819580078,\n","           66.68780517578125,\n","           182.41531372070312,\n","           138.31463623046875],\n"," 'labels': 3,\n"," 'scores': 0.21083638072013855}\n","================== batch ==================\n","filename:  communication_15--messages-0-6_d2d8.jpg\n","outputs\n","[{'boxes': tensor([], size=(0, 4)),\n","  'labels': tensor([], dtype=torch.int64),\n","  'scores': tensor([])}]\n","inference result\n","{'boxes': None, 'labels': None, 'scores': None}\n","================== batch ==================\n","filename:  communication_16--messenger-free-texting-app-1-16_6e18.jpg\n","outputs\n","[{'boxes': tensor([], size=(0, 4)),\n","  'labels': tensor([], dtype=torch.int64),\n","  'scores': tensor([])}]\n","inference result\n","{'boxes': None, 'labels': None, 'scores': None}\n","================== batch ==================\n","filename:  communication_20--call-free-free-online-1-51_ce2e.jpg\n","outputs\n","[{'boxes': tensor([[ 81.3714, 123.6833, 144.7532, 151.0545]]),\n","  'labels': tensor([3]),\n","  'scores': tensor([0.0578])}]\n","inference result\n","{'boxes': [81.37140655517578,\n","           123.68331909179688,\n","           144.75315856933594,\n","           151.05445861816406],\n"," 'labels': 3,\n"," 'scores': 0.057776909321546555}\n","================== batch ==================\n","filename:  communication_24--Line-0-7_308e.jpg\n","outputs\n","[{'boxes': tensor([], size=(0, 4)),\n","  'labels': tensor([], dtype=torch.int64),\n","  'scores': tensor([])}]\n","inference result\n","{'boxes': None, 'labels': None, 'scores': None}\n","================== batch ==================\n","filename:  communication_24--Line-1-45_e9fc.jpg\n","outputs\n","[{'boxes': tensor([[176.8472, 342.3802, 209.8840, 396.7265]]),\n","  'labels': tensor([3]),\n","  'scores': tensor([0.0543])}]\n","inference result\n","{'boxes': [176.8472137451172,\n","           342.3802490234375,\n","           209.884033203125,\n","           396.7265319824219],\n"," 'labels': 3,\n"," 'scores': 0.05433432385325432}\n","================== batch ==================\n","filename:  communication_27--boss-revolution-3-43_dda6.jpg\n","outputs\n","[{'boxes': tensor([[ 98.3425, 320.3226, 126.4231, 366.1132]]),\n","  'labels': tensor([3]),\n","  'scores': tensor([0.1055])}]\n","inference result\n","{'boxes': [98.34248352050781,\n","           320.3226013183594,\n","           126.42314910888672,\n","           366.11322021484375],\n"," 'labels': 3,\n"," 'scores': 0.10553018748760223}\n","================== batch ==================\n","filename:  communication_29--Messenger-0-19_1989.jpg\n","outputs\n","[{'boxes': tensor([], size=(0, 4)),\n","  'labels': tensor([], dtype=torch.int64),\n","  'scores': tensor([])}]\n","inference result\n","{'boxes': None, 'labels': None, 'scores': None}\n","================== batch ==================\n","filename:  communication_29--Messenger-0-26_23dc.jpg\n","outputs\n","[{'boxes': tensor([[101.5660,  40.6264, 180.1337, 157.1765]]),\n","  'labels': tensor([3]),\n","  'scores': tensor([0.9417])}]\n","inference result\n","{'boxes': [101.56598663330078,\n","           40.62641143798828,\n","           180.13365173339844,\n","           157.17648315429688],\n"," 'labels': 3,\n"," 'scores': 0.9416988492012024}\n","================== batch ==================\n","filename:  communication_29--Messenger-1-11_8b7e.jpg\n","outputs\n","[{'boxes': tensor([], size=(0, 4)),\n","  'labels': tensor([], dtype=torch.int64),\n","  'scores': tensor([])}]\n","inference result\n","{'boxes': None, 'labels': None, 'scores': None}\n","================== batch ==================\n","filename:  communication_3--textnow-0-16_24d9.jpg\n","outputs\n","[{'boxes': tensor([], size=(0, 4)),\n","  'labels': tensor([], dtype=torch.int64),\n","  'scores': tensor([])}]\n","inference result\n","{'boxes': None, 'labels': None, 'scores': None}\n","================== batch ==================\n","filename:  communication_3--textnow-0-25_8b45.jpg\n","outputs\n","[{'boxes': tensor([], size=(0, 4)),\n","  'labels': tensor([], dtype=torch.int64),\n","  'scores': tensor([])}]\n","inference result\n","{'boxes': None, 'labels': None, 'scores': None}\n","================== batch ==================\n","filename:  communication_3--textnow-5-26_21c0.jpg\n","outputs\n","[{'boxes': tensor([], size=(0, 4)),\n","  'labels': tensor([], dtype=torch.int64),\n","  'scores': tensor([])}]\n","inference result\n","{'boxes': None, 'labels': None, 'scores': None}\n","================== batch ==================\n","filename:  entertainment_19--Face-Secret-Master-0-7_b147.jpg\n","outputs\n","[{'boxes': tensor([], size=(0, 4)),\n","  'labels': tensor([], dtype=torch.int64),\n","  'scores': tensor([])}]\n","inference result\n","{'boxes': None, 'labels': None, 'scores': None}\n","================== batch ==================\n","filename:  entertainment_19--Face-Secret-Master-4-30_cc29.jpg\n","outputs\n","[{'boxes': tensor([], size=(0, 4)),\n","  'labels': tensor([], dtype=torch.int64),\n","  'scores': tensor([])}]\n","inference result\n","{'boxes': None, 'labels': None, 'scores': None}\n","================== batch ==================\n","filename:  entertainment_19--Face-Secret-Master-5-15_5310.jpg\n","outputs\n","[{'boxes': tensor([], size=(0, 4)),\n","  'labels': tensor([], dtype=torch.int64),\n","  'scores': tensor([])}]\n","inference result\n","{'boxes': None, 'labels': None, 'scores': None}\n","================== batch ==================\n","filename:  entertainment_6--Bitmoj-6-48_91ee.jpg\n","outputs\n","[{'boxes': tensor([], size=(0, 4)),\n","  'labels': tensor([], dtype=torch.int64),\n","  'scores': tensor([])}]\n","inference result\n","{'boxes': None, 'labels': None, 'scores': None}\n","================== batch ==================\n","filename:  music_11--Edijing-mix-1-35_ed63.jpg\n","outputs\n","[{'boxes': tensor([], size=(0, 4)),\n","  'labels': tensor([], dtype=torch.int64),\n","  'scores': tensor([])}]\n","inference result\n","{'boxes': None, 'labels': None, 'scores': None}\n","================== batch ==================\n","filename:  music_11--Edijing-mix-5-11_ec94.jpg\n","outputs\n","[{'boxes': tensor([], size=(0, 4)),\n","  'labels': tensor([], dtype=torch.int64),\n","  'scores': tensor([])}]\n","inference result\n","{'boxes': None, 'labels': None, 'scores': None}\n","================== batch ==================\n","filename:  music_15--Volume-Booster-0-46_a68c.jpg\n","outputs\n","[{'boxes': tensor([], size=(0, 4)),\n","  'labels': tensor([], dtype=torch.int64),\n","  'scores': tensor([])}]\n","inference result\n","{'boxes': None, 'labels': None, 'scores': None}\n","================== batch ==================\n","filename:  music_2--Youtube-Music-0-8_89fe.jpg\n","outputs\n","[{'boxes': tensor([], size=(0, 4)),\n","  'labels': tensor([], dtype=torch.int64),\n","  'scores': tensor([])}]\n","inference result\n","{'boxes': None, 'labels': None, 'scores': None}\n","================== batch ==================\n","filename:  music_2--Youtube-Music-3-14_3f38.jpg\n","outputs\n","[{'boxes': tensor([], size=(0, 4)),\n","  'labels': tensor([], dtype=torch.int64),\n","  'scores': tensor([])}]\n","inference result\n","{'boxes': None, 'labels': None, 'scores': None}\n","================== batch ==================\n","filename:  music_21--TuneIn-Radio-0-40_582f.jpg\n","outputs\n","[{'boxes': tensor([], size=(0, 4)),\n","  'labels': tensor([], dtype=torch.int64),\n","  'scores': tensor([])}]\n","inference result\n","{'boxes': None, 'labels': None, 'scores': None}\n","================== batch ==================\n","filename:  music_22--Starmaker-8-1_9e27.jpg\n","outputs\n","[{'boxes': tensor([], size=(0, 4)),\n","  'labels': tensor([], dtype=torch.int64),\n","  'scores': tensor([])}]\n","inference result\n","{'boxes': None, 'labels': None, 'scores': None}\n","================== batch ==================\n","filename:  music_24--Simple-Radio-0-20_589f.jpg\n","outputs\n","[{'boxes': tensor([], size=(0, 4)),\n","  'labels': tensor([], dtype=torch.int64),\n","  'scores': tensor([])}]\n","inference result\n","{'boxes': None, 'labels': None, 'scores': None}\n","================== batch ==================\n","filename:  music_27--Groovepad-3-19_12c6.jpg\n","outputs\n","[{'boxes': tensor([], size=(0, 4)),\n","  'labels': tensor([], dtype=torch.int64),\n","  'scores': tensor([])}]\n","inference result\n","{'boxes': None, 'labels': None, 'scores': None}\n","================== batch ==================\n","filename:  music_30--Music-Bass-Equalizer-0-6_6cef.jpg\n","outputs\n","[{'boxes': tensor([], size=(0, 4)),\n","  'labels': tensor([], dtype=torch.int64),\n","  'scores': tensor([])}]\n","inference result\n","{'boxes': None, 'labels': None, 'scores': None}\n","================== batch ==================\n","filename:  music_7--Shazam-1-13_4d86.jpg\n","outputs\n","[{'boxes': tensor([], size=(0, 4)),\n","  'labels': tensor([], dtype=torch.int64),\n","  'scores': tensor([])}]\n","inference result\n","{'boxes': None, 'labels': None, 'scores': None}\n","================== batch ==================\n","filename:  music_9--AudioMack-0-24_dcf6.jpg\n","outputs\n","[{'boxes': tensor([], size=(0, 4)),\n","  'labels': tensor([], dtype=torch.int64),\n","  'scores': tensor([])}]\n","inference result\n","{'boxes': None, 'labels': None, 'scores': None}\n"]}]},{"cell_type":"code","source":["# # write inference result\n","# with open('./inference_result.json', 'w') as fp:\n","#   json.dump(inference_result, fp)"],"metadata":{"id":"WIdPXpVeQh4T"},"id":"WIdPXpVeQh4T","execution_count":null,"outputs":[]}]}